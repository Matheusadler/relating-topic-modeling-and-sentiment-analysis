{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mathe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from time import time\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining parameters\n",
    "n_features = 10000\n",
    "n_components = 15\n",
    "n_top_word = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print topics\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>User_name</th>\n",
       "      <th>Time</th>\n",
       "      <th>Location</th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>UK</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>ÃƒÆ’Ã‚Å“T: 36.319708,-82.363649</td>\n",
       "      <td>As news of the regionÃƒâ€šÃ‚â€™s first confirm...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>35.926541,-78.753267</td>\n",
       "      <td>Cashier at grocery store was sharing his insig...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Atlanta, GA USA</td>\n",
       "      <td>Due to COVID-19 our retail store and classroom...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  User_name        Time                         Location  \\\n",
       "0           1          2  16-03-2020                               UK   \n",
       "1           3          4  16-03-2020                              NaN   \n",
       "2           5          6  16-03-2020  ÃƒÆ’Ã‚Å“T: 36.319708,-82.363649   \n",
       "3           6          7  16-03-2020             35.926541,-78.753267   \n",
       "4           8          9  16-03-2020                  Atlanta, GA USA   \n",
       "\n",
       "                                                text Sentiment  \n",
       "0  advice Talk to your neighbours family to excha...  Positive  \n",
       "1  My food stock is not the only one which is emp...  Positive  \n",
       "2  As news of the regionÃƒâ€šÃ‚â€™s first confirm...  Positive  \n",
       "3  Cashier at grocery store was sharing his insig...  Positive  \n",
       "4  Due to COVID-19 our retail store and classroom...  Positive  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading dataset\n",
    "print(\"Loading dataset...\")\n",
    "dataset = pd.read_csv('tweets-covid-positive.csv', engine='python')\n",
    "data_samples = dataset.iloc[1:,0]\n",
    "n_samples = len(data_samples)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StopWords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['advice talk neighbours family exchange phone numbers create contact list phone numbers neighbours schools employer chemist gp set online shopping accounts poss adequate supplies regular meds order',\n",
       " 'food stock one empty please panic enough food everyone take need stay calm stay safe covid france covid covid coronavirus confinement confinementotal confinementgeneral',\n",
       " 'news region first confirmed covid case came sullivan county last week people flocked area stores purchase cleaning supplies hand sanitizer food toilet paper goods tim dodson reports',\n",
       " 'cashier grocery store sharing insights covid prove credibility commented civics class know talking',\n",
       " 'due covid retail store classroom atlanta open walk business classes next two weeks beginning monday march continue process online phone orders normal thank understanding']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_comment = []\n",
    "\n",
    "# Preprocessing\n",
    "for i in range(dataset.shape[0]):\n",
    "    comment = re.sub('[^a-zA-Z]',' ',dataset['text'][i]) # Remove non-letters\n",
    "    comment = comment.split(\"http\", 1)[0] # Remove address from string\n",
    "    comment = comment.lower() # Set lower case\n",
    "    comment = comment.split() # Divide into a list\n",
    "    comment = [word for word in comment if not word in stopwords.words('english')] # Select important words\n",
    "    comment =' '.join(comment)\n",
    "    new_comment.append(comment)\n",
    "\n",
    "new_comment[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF...\n",
      "tf-idf features extracted!\n"
     ]
    }
   ],
   "source": [
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words=stopwords.words('english'))\n",
    "tfidf = tfidf_vectorizer.fit_transform(new_comment)\n",
    "\n",
    "print(\"tf-idf features extracted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "tf features for LDA extraction is completed!\n"
     ]
    }
   ],
   "source": [
    "# Use tf features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words=stopwords.words('english'))\n",
    "tf = tf_vectorizer.fit_transform(new_comment)\n",
    "\n",
    "print(\"tf features for LDA extraction is completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=17341 and n_features=10000...\n",
      "done in 4.315s.\n",
      "\n",
      "Topics in NMF model (Frobenius norm):\n",
      "Topic #0: covid pandemic retail uk virus spread due\n",
      "Topic #1: store grocery get go like employees retail\n",
      "Topic #2: coronavirus toiletpaper pandemic quarantine right outbreak stophoarding\n",
      "Topic #3: food demand stock need bank panic supply\n",
      "Topic #4: online shopping delivery shop time support amazon\n",
      "Topic #5: supermarket staff get go like shelves one\n",
      "Topic #6: amp staff delivery support health stores keep\n",
      "Topic #7: sanitizer hand masks hands use alcohol wash\n",
      "Topic #8: prices time get price masks free oil\n",
      "Topic #9: workers thank drivers staff delivery care nurses\n",
      "Topic #10: consumer pandemic business new behavior time consumers\n",
      "Topic #11: people many need like going also buying\n",
      "Topic #12: home stay safe keep work please hands\n",
      "Topic #13: help need us please local support get\n",
      "Topic #14: paper toilet toiletpaper like get would rolls\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mathe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features, n_samples=17341 and n_features=10000...\n",
      "done in 13.253s.\n",
      "\n",
      "Topics in NMF model (generalized Kullback-Leibler divergence):\n",
      "Topic #0: covid pandemic store retail good one open\n",
      "Topic #1: store grocery coronavirus go like socialdistancing going\n",
      "Topic #2: coronavirus pandemic toiletpaper good corona new stophoarding\n",
      "Topic #3: food stock demand panic buy local supply\n",
      "Topic #4: online shopping delivery shop time free amazon\n",
      "Topic #5: supermarket staff shelves go like local going\n",
      "Topic #6: amp covid health best retail measures public\n",
      "Topic #7: sanitizer hand masks use gloves hands make\n",
      "Topic #8: prices price high time oil market low\n",
      "Topic #9: workers thank pandemic health employees care staff\n",
      "Topic #10: consumer pandemic business new read impact consumers\n",
      "Topic #11: people covid coronavirus many stayathome buying coronavirusoutbreak\n",
      "Topic #12: home stay safe keep covid work everyone\n",
      "Topic #13: help covid us please need support let\n",
      "Topic #14: get need like paper toilet covid toiletpaper\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
    "      \"tf-idf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features, n_samples=17341 and n_features=10000...\n",
      "done in 29.006s.\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: supermarket covid coronavirus people get food like\n",
      "Topic #1: covid consumer prices coronavirus amp us social\n",
      "Topic #2: prices covid market high coronavirus oil free\n",
      "Topic #3: coronavirus toiletpaper paper toilet price stayhomesavelives covid\n",
      "Topic #4: online covid shopping food stay home amp\n",
      "Topic #5: covid store grocery coronavirus working supermarket people\n",
      "Topic #6: consumer data behavior consumers industry read covid\n",
      "Topic #7: store grocery covid coronavirus workers amp employees\n",
      "Topic #8: workers supermarket staff covid thank stop food\n",
      "Topic #9: sanitizer hand coronavirus masks covid use hands\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the LDA model\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fiting LSA model\n",
      "\n",
      "Topics in LSA model:\n",
      "Topic #0: covid coronavirus store grocery food supermarket amp\n",
      "Topic #1: covid consumer prices food online supermarket shopping\n",
      "Topic #2: store grocery workers covid thank employees retail\n",
      "Topic #3: food amp supermarket people online shopping need\n",
      "Topic #4: online shopping amp home shop delivery time\n",
      "Topic #5: food sanitizer store grocery hand online demand\n",
      "Topic #6: amp hand sanitizer consumer prices workers masks\n",
      "Topic #7: hand sanitizer people supermarket online shopping food\n",
      "Topic #8: prices people get time need like help\n",
      "Topic #9: workers prices consumer thank us pandemic online\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the LSA model\n",
    "print(\"Fiting LSA model\")\n",
    "\n",
    "lsa = TruncatedSVD(n_components=n_components, n_iter=40, tol=0.01)\n",
    "\n",
    "lsa.fit(tf)\n",
    "\n",
    "print(\"\\nTopics in LSA model:\")\n",
    "\n",
    "print_top_words(lsa, tf_feature_names, n_top_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
