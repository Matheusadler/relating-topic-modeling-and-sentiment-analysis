{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import libs.utils as utl\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the embedding generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL of the pre-trained Universal Sentence Encoder (USE) model \n",
    "module_url = 'https://tfhub.dev/google/universal-sentence-encoder-large/4'\n",
    "# Create a Keras Layer using the Universal Sentence Encoder (USE) model\n",
    "USE_embed = hub.KerasLayer(module_url, trainable=False, name='USE_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading the dataset from CSV files for training, testing, and validation\n",
    "train_dataframe = pd.read_csv(\"data/Corona_NLP_train.csv\", encoding = \"ISO-8859-1\")\n",
    "train_dataframe[\"Sentiment\"] = train_dataframe[\"Sentiment\"].astype('category')\n",
    "\n",
    "test_dataframe = pd.read_csv(\"data/Corona_NLP_test.csv\", encoding = \"ISO-8859-1\")\n",
    "test_dataframe[\"Sentiment\"] = test_dataframe[\"Sentiment\"].astype('category')\n",
    "\n",
    "valid_dataframe = pd.read_csv(\"data/Corona_NLP_valid.csv\", encoding = \"ISO-8859-1\")\n",
    "valid_dataframe[\"Sentiment\"] = valid_dataframe[\"Sentiment\"].astype('category')\n",
    "\n",
    "# Create a dictionary mapping category codes to their original string labels\n",
    "label_dict = dict(enumerate(train_dataframe[\"Sentiment\"].cat.categories))\n",
    "\n",
    "# Displaying information about the dataset using Seaborn\n",
    "sns.catplot(x=\"Sentiment\", kind=\"count\", data=train_dataframe, aspect=2).set_xticklabels(rotation=90)\n",
    "sns.catplot(x=\"Sentiment\", kind=\"count\", data=valid_dataframe, aspect=2).set_xticklabels(rotation=90)\n",
    "sns.catplot(x=\"Sentiment\", kind=\"count\", data=test_dataframe, aspect=2).set_xticklabels(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding tweet text for embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the tweet text data from the dataframes\n",
    "train_x = train_dataframe['OriginalTweet'].to_numpy()\n",
    "valid_x = valid_dataframe['OriginalTweet'].to_numpy()\n",
    "test_x = test_dataframe['OriginalTweet'].to_numpy()\n",
    "\n",
    "# Encoding the categorical labels for sentiment\n",
    "train_y = train_dataframe[\"Sentiment\"].cat.codes.to_numpy()\n",
    "valid_y = valid_dataframe[\"Sentiment\"].cat.codes.to_numpy()\n",
    "test_y = test_dataframe[\"Sentiment\"].cat.codes.to_numpy()\n",
    "\n",
    "# Generating random batches of data for training, validation, and testing\n",
    "train_batches = utl.gen_random_batches(train_x, train_y, batch_size = 100)\n",
    "valid_batches = utl.gen_random_batches(valid_x, valid_y, batch_size = 100)\n",
    "test_batches = utl.gen_random_batches(test_x, test_y, batch_size = 100)\n",
    "\n",
    "print(\"Number of training batches:\", len(train_batches))\n",
    "print(\"Number of validation batches:\", len(valid_batches))\n",
    "\n",
    "# Encoding the input tweet text data using the Universal Sentence Encoder (USE) model\n",
    "print(\"Shape of X in the training batch before encoding:\", train_batches[0][0].shape)\n",
    "for index, batch in enumerate(train_batches):\n",
    "    (batch_x, batch_y) = batch\n",
    "    embeddings = USE_embed(batch_x.astype('str').tolist())\n",
    "    train_batches[index] = (embeddings['outputs'].numpy(), batch_y)\n",
    "print(\"Shape of X in the training batch after encoding:\", train_batches[0][0].shape)\n",
    "\n",
    "print(\"Shape of X in the validation batch before encoding:\", valid_batches[0][0].shape)\n",
    "for index, batch in enumerate(valid_batches):\n",
    "    (batch_x, batch_y) = batch\n",
    "    embeddings = USE_embed(batch_x.astype('str').tolist())\n",
    "    valid_batches[index] = (embeddings['outputs'].numpy(), batch_y)\n",
    "print(\"Shape of X in the validation batch after encoding:\", valid_batches[0][0].shape)\n",
    "\n",
    "\n",
    "print(\"Shape of X in the test batch before encoding:\", valid_batches[0][0].shape)\n",
    "for index, batch in enumerate(test_batches):\n",
    "    (batch_x, batch_y) = batch\n",
    "    embeddings = USE_embed(batch_x.astype('str').tolist())\n",
    "    test_batches[index] = (embeddings['outputs'].numpy(), batch_y)\n",
    "print(\"Shape of X in the test batch after encoding:\", valid_batches[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the neural network model using the Sequential API in Keras\n",
    "model = Sequential([\n",
    "  layers.Dense(1014, activation='relu'),\n",
    "  layers.Dense(512, activation='relu'),\n",
    "  layers.Dense(len(label_dict))\n",
    "])\n",
    "\n",
    "# Building the model with an input shape of (None, 512)\n",
    "model.build((None,512))\n",
    "\n",
    "# Printing a summary of the model architecture\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the loss function and evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function: Sparse Categorical Crossentropy\n",
    "train_loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Optimizer: Adam optimizer with a learning rate of 0.01\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# Metrics for training:\n",
    "#   - Sparse Categorical Accuracy: Measures accuracy for integer labels\n",
    "#   - Mean: Computes the mean of elements for tracking the training loss\n",
    "train_acu_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "train_loss_acc = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "# Metric for validation:\n",
    "valid_acu_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# Custom metrics class calculates F1, Recall, and Precision\n",
    "#   - To add a new metric, refer to the CustomMetrics class in the utils.py file\n",
    "custom_metrics = utl.CustomMetrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store results for each epoch\n",
    "list_train_loss_results = []\n",
    "list_train_acc_results = []\n",
    "list_valid_acc_results = []\n",
    "list_valid_rec_results = []\n",
    "list_valid_pre_results = []\n",
    "list_valid_f1_results = []\n",
    "\n",
    "# Dictionary to track the best scores for F1, Recall, and Precision\n",
    "best_score = {\"F1\": 0, \"Recall\": 0, \"Precision\": 0}\n",
    "\n",
    "# TensorFlow function for training a single batch\n",
    "@tf.function\n",
    "def train_step(batch_x, batch_y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(batch_x)\n",
    "        loss = train_loss_obj(batch_y, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_loss_acc(loss)\n",
    "    train_acu_acc(batch_y, predictions)\n",
    "\n",
    "# Maximum number of training epochs\n",
    "MAX_EPOCHS = 40\n",
    "for epoch in tqdm(range(0, MAX_EPOCHS)):\n",
    "    \n",
    "   \n",
    "    # Training with the training batches\n",
    "    for batch in train_batches:\n",
    "        (batch_x, batch_y) = batch\n",
    "        train_step(batch_x, batch_y)\n",
    "    \n",
    "    # Evaluation with the validation batches\n",
    "    for batch in valid_batches:\n",
    "        (batch_x, batch_y) = batch\n",
    "        predictions = model(batch_x)\n",
    "        \n",
    "        # Updating TensorFlow accuracy metric\n",
    "        valid_acu_acc(batch_y, predictions)\n",
    "        \n",
    "        # Getting predictions with the highest confidence for custom metrics\n",
    "        predictions = tf.argmax(tf.nn.softmax(predictions), axis=1)\n",
    "        custom_metrics.feed(batch_y, predictions)\n",
    "        \n",
    "       \n",
    "    \n",
    "    # Saving results in lists for later visualization\n",
    "    list_train_loss_results.append(train_loss_acc.result().numpy())\n",
    "    list_train_acc_results.append(train_acu_acc.result().numpy())\n",
    "    list_valid_acc_results.append(valid_acu_acc.result().numpy())\n",
    "    \n",
    "    # Extracting custom metric results\n",
    "    custom_results = custom_metrics.results()\n",
    "     \n",
    "    # Saving custom metric results in lists for later visualization\n",
    "    list_valid_rec_results.append(custom_results[\"Recall\"])\n",
    "    list_valid_pre_results.append(custom_results[\"Precision\"])\n",
    "    list_valid_f1_results.append(custom_results[\"F1\"])\n",
    "    \n",
    "    # Checking if the best result has been achieved\n",
    "    if custom_results[\"F1\"] > best_score[\"F1\"]:\n",
    "        best_score = custom_results\n",
    "        model.save_weights(\"/model_save/model\", save_format='tf')\n",
    "    \n",
    "    # Resetting the metrics\n",
    "    train_loss_acc.reset_states()\n",
    "    train_acu_acc.reset_states()\n",
    "    valid_acu_acc.reset_states()\n",
    "    custom_metrics.reset_states()\n",
    "\n",
    "    \n",
    "print(\"best result:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying experiment graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training loss over epochs\n",
    "sns.lineplot(data=list_train_loss_results).set_title('Loss training')\n",
    "plt.pause(0.1)\n",
    "\n",
    "# Plotting the training accuracy over epochs\n",
    "sns.lineplot(data=list_train_acc_results).set_title('Accuracy training')\n",
    "plt.pause(0.1)\n",
    "\n",
    "# Plotting the validation accuracy over epochs\n",
    "sns.lineplot(data=list_valid_acc_results).set_title('Accuracy validation')\n",
    "plt.pause(0.1)\n",
    "\n",
    "# Plotting the validation recall over epochs\n",
    "sns.lineplot(data=list_valid_rec_results).set_title('Recall validation')\n",
    "plt.pause(0.1)\n",
    "\n",
    "# Plotting the validation precision over epochs\n",
    "sns.lineplot(data=list_valid_pre_results).set_title('Precision validation')\n",
    "plt.pause(0.1)\n",
    "\n",
    "# Plotting the validation F1 score over epochs\n",
    "sns.lineplot(data=list_valid_f1_results).set_title('F1 validation')\n",
    "plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model with the best F1 score\n",
    "model.load_weights(\"/model_save/model\")\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "cf_matrix = utl.calcule_confusion_matrix(model, valid_batches)\n",
    "\n",
    "# Creating a subplot with the specified dimensions\n",
    "fig_dims = (20, 20)\n",
    "fig, ax = plt.subplots(figsize=fig_dims)\n",
    "\n",
    "# Using Seaborn to create a heatmap for the confusion matrix\n",
    "sns.heatmap(cf_matrix,  annot=True, fmt=\"d\", linewidths=.5, xticklabels=label_dict.values(), yticklabels=label_dict.values(), cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model with the best F1 score\n",
    "model.load_weights(\"/model_save/model\")\n",
    "\n",
    "# Resetting the custom metrics states before evaluation\n",
    "custom_metrics.reset_states()\n",
    "\n",
    "# Evaluation with the batches from the test dataset\n",
    "for batch in test_batches:\n",
    "    (batch_x, batch_y) = batch\n",
    "    predictions = model(batch_x)\n",
    "\n",
    "    # Getting predictions with the highest confidence for custom metrics\n",
    "    predictions = tf.argmax(tf.nn.softmax(predictions), axis=1)\n",
    "    custom_metrics.feed(batch_y, predictions)\n",
    "\n",
    "# Printing the results of custom metrics (F1, Recall, Precision) for the test dataset\n",
    "print(custom_metrics.results())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
